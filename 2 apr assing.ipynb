{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdd46a5-861f-40ab-856a-0406dfc1e058",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 1\n",
    "Grid search cross-validation is a technique used in machine learning to systematically search for the optimal hyperparameters of a model. Hyperparameters are parameters that are not learned from the data but set by the user before the learning process begins.\n",
    "\n",
    "GridSearch cv works:\n",
    "    \n",
    "(1)define the model\n",
    "(2)define the hyperparameter space\n",
    "(3)define evaluation metric\n",
    "(4)grid search \n",
    "(5)model selection\n",
    "(6)retrain with best parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8819a8-7d46-426b-b4b9-72e470aa03eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 2\n",
    "GRID SEARCH CV :\n",
    "(1) It performs an exhaustive search over all possible combination of hyperparameter values specified in predefined grid\n",
    "(2) It systematically evaluates the models performance for each combination using cross validation.\n",
    "\n",
    "RANDOM SEARCH CV:\n",
    "(1) It randomly samples a specifies number of combination from the hyperparameter space\n",
    "(2) It allows you to define a probability distruction for each hyperparameter insted of specifying discrete values or ranges.\n",
    "\n",
    "Choosing between grid search cv and randoized search cv depends on several factors:\n",
    "\n",
    "hyperparameter space\n",
    "computational space\n",
    "time constraints\n",
    "exploration vs exploitation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b26b43-39b7-4100-92c1-51fbefc9d6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 3\n",
    "Data leakage refers to the situation where information from outside the training data is inadvertently or improperly used during the model training process, leading to over-optimistic performance estimates and potentially biased or unreliable models. It occurs when information that would not be available in real-world scenarios is used to make predictions or influence the model's learning process.\n",
    "\n",
    "Data leakage is a problem in machine learning because it can lead to overly optimistic performance metrics, making the model appear more accurate than it actually is. When the model is deployed in a real-world setting, it may fail to perform as expected, resulting in poor generalization and unreliable predictions.\n",
    "\n",
    "example - \n",
    "Data leakage is problematic because it can result in models that fail to generalize well to unseen data, leading to poor performance and unreliable predictions. To mitigate data leakage, it is important to carefully preprocess the data, ensure proper separation of training and validation sets, and avoid using any information that would not be available in real-world scenarios during the model training process.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512a6bc0-b3f9-4de8-b267-74a1238d88d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 4\n",
    "To prevent data leakage and ensure the reliability and generalization of your machine learning model, consider the following strategies:\n",
    "(1) Proper Data Splitting \n",
    "(2)Temporal Split\n",
    "(3)Feature Engineering Awareness\n",
    "(4)Data processing\n",
    "(5)CrossValidation Techniques\n",
    "(6)Domain Knowladge and common sense\n",
    "(7)Regular Monitoring and auditing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3974a32b-a8a8-424e-86f7-42fa20a58ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 5\n",
    "A confusion matrix is a tabular representation that summarizes the performance of a classification model by showing the counts of true positive, true negative, false positive, and false negative prediction. it is widely used to evaluates the performance of a binary and multi class classification models.\n",
    "\n",
    "The confusion matrix provides valuable insights into the performance of a classification model :\n",
    "\n",
    "(1) Accuracy \n",
    "(2) Precision\n",
    "(3) Recall\n",
    "(4)F1_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c20c344-d573-424e-b8be-faa57c9f4a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 6\n",
    "In context of a confusion matrix, precision is calculated by dividing the number of true positive(TP) prediction by the sum of true positive and false positive prediction. it represents the models ability to minimize false positive errors.\n",
    "\n",
    "\n",
    "In confusion matrix , recall is calculated by dividing the number of true positive prediction by the sum of true positive and false negative prediction. it indicates the models ability to identify all positive instances correctly, without missing any.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0dcbf2-7ace-430d-aedc-7462a5c1d27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 7\n",
    "Interpreting a confusion matrix can provide insights into the types of errors your model is making and help identify its strengths and weaknesses. Here's how you can interpret a confusion matrix to understand the model's errors:\n",
    "(1) True Positive\n",
    "(2) True Negative \n",
    "(3) False Positive \n",
    "(4) False Negative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ac330e-3e90-4c95-8bab-f9ff1db75ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 8\n",
    "Several common metrics can be derived from a confusion matrix to assess the performance of a classification model. Here are some widely used metrics and their calculations:\n",
    "    \n",
    "(1) Accuracy:\n",
    "         (TP+TN)/(TP+TN+FP+FN)\n",
    "        \n",
    "(2) Precision :\n",
    "    TP/(TP+FP)\n",
    "    \n",
    "(3) Recall:\n",
    "           TP/(TP+FN)\n",
    "\n",
    "(4)F1-SCORE:\n",
    "    2*(Precision*Recall)/(Precision+Recall)\n",
    "    \n",
    "(5)Specificity (True Negative Rate):\n",
    "    TN/(TN+FP)\n",
    "    \n",
    "(6) False positive rate:\n",
    "    FP/(FP+TN)\n",
    "    \n",
    "(7) Balanced Accuracy:\n",
    "    (Recall Positive + Recall Negative)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0c704b-5d32-40f0-8d48-c2183b7908ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 9\n",
    "\n",
    "Accuracy is calculated as the ratio of correct predictions (true positives and true negatives) to the total number of instances:\n",
    "\n",
    "(TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "how accuracy relates to the values in the confusion matrix:\n",
    "\n",
    "(1)True Positives (TP): These are the instances correctly predicted as positive by the model. They contribute to the numerator of the accuracy calculation, as TP increases the number of correct predictions.\n",
    "\n",
    "(2)True Negatives (TN): These are the instances correctly predicted as negative by the model. Similar to TP, TN contributes to the numerator of the accuracy calculation, as TN increases the number of correct predictions.\n",
    "\n",
    "(3)False Positives (FP): These are the instances incorrectly predicted as positive by the model. FP increases the denominator of the accuracy calculation, reducing the accuracy value since it represents incorrect predictions.\n",
    "\n",
    "(4)False Negatives (FN): These are the instances incorrectly predicted as negative by the model. Similar to FP, FN increases the denominator of the accuracy calculation, lowering the accuracy value since it represents incorrect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7c97c9-9ac9-4d7e-a06a-f7a0da916d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 10\n",
    "\n",
    "A confusion matrix can be used as a tool to identify potential biases or limitations in your machine learning model. Here are some ways to leverage the confusion matrix for this purpose:\n",
    "\n",
    "(1) Class Imbalance\n",
    "(2) False Positive and False Negative rates \n",
    "(3) Errors Patterns \n",
    "(4) Performance Discrepancies\n",
    "(5) External Factors and Domain Knowladge"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
